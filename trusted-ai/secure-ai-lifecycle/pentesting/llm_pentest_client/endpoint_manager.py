"""
Endpoint management for LLM Penetration Testing.

Handles creation and management of LLM endpoints (OpenAI-compatible and custom).
"""

from typing import Dict, Any, List, Optional, Literal, TYPE_CHECKING

if TYPE_CHECKING:
    from .client import LLMPentestClient


class EndpointManager:
    """
    Manages LLM endpoints for penetration testing.

    Supports both OpenAI-compatible endpoints and fully custom REST API endpoints.
    """

    def __init__(self, client: "LLMPentestClient"):
        """
        Initialize the endpoint manager.

        Args:
            client: Parent LLMPentestClient instance.
        """
        self.client = client

    def create_openai(
        self,
        api_key: str,
        identifier: str,
        display_name: Optional[str] = None,
        provider: str = "OpenAI",
        base_url: Optional[str] = None,
        pentest_url: Optional[str] = None,
        pentest_api_key: Optional[str] = None,
        pentest_model: str = "gpt-3.5-turbo",
        project_ids: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Create an OpenAI-compatible LLM endpoint.

        This is used for OpenAI endpoints or any endpoint that follows the same
        signature as OpenAI's v1/chat/completions endpoints.

        Args:
            api_key: API key for the endpoint.
            identifier: Unique identifier for the endpoint.
            display_name: Human-readable name for the endpoint.
            provider: Provider name (default: "OpenAI").
            base_url: Base URL for the API (optional, defaults to OpenAI's URL).
            pentest_url: Override URL for pentest requests (optional).
            pentest_api_key: Override API key for pentest requests (optional).
            pentest_model: Model to use for pentest (default: "gpt-3.5-turbo").
            project_ids: List of project IDs to associate with (optional).

        Returns:
            Created endpoint details including resource_instance_id.

        Example:
            endpoint = client.endpoints.create_openai(
                api_key="sk-...",
                identifier="my-gpt4-endpoint",
                display_name="GPT-4 Production",
                pentest_model="gpt-4o"
            )
            resource_id = endpoint['added_resources'][0]['resource_instance_id']
        """
        resource_data = {
            "api_key": api_key,
            "endpoint_identifier": identifier,
            "provider": provider
        }

        if base_url:
            resource_data["base_url"] = base_url

        if pentest_url or pentest_api_key:
            resource_data["pentest_connection_details"] = {
                "model": pentest_model
            }
            if pentest_url:
                resource_data["pentest_connection_details"]["pentest_url"] = pentest_url
            if pentest_api_key:
                resource_data["pentest_connection_details"]["pentest_api_key"] = pentest_api_key

        data = {
            "resource_type": "OpenAIEndpoint",
            "display_name": display_name or f"OpenAI Endpoint ({identifier})",
            "resource_data": resource_data
        }

        if project_ids:
            data["project_ids"] = project_ids

        endpoint = f"/v1/inventory/customer/{self.client.customer_id}/resources/llm-endpoint"
        return self.client.make_request(endpoint, method="POST", data=data)

    def create_custom(
        self,
        identifier: str,
        pentest_url: str,
        headers: Dict[str, str],
        body: Dict[str, Any],
        response_jsonpaths: List[str],
        display_name: Optional[str] = None,
        method: Literal["POST", "PUT"] = "POST",
        response_type: Literal["json", "text", "ndjson"] = "json",
        response_error_values: Optional[List[str]] = None,
        project_ids: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Create a custom REST API LLM endpoint.

        Use this when your endpoint doesn't conform to the OpenAI API structure.
        You must fully specify how to call your API during pentests.

        Args:
            identifier: Unique identifier for the endpoint.
            pentest_url: Full URL of the API endpoint.
            headers: Request headers including authentication.
            body: Request body template (must include <<PROMPT>> placeholder).
            response_jsonpaths: JSONPath expressions to extract response text.
                All matches will be concatenated.
            display_name: Human-readable name for the endpoint.
            method: HTTP method (POST or PUT).
            response_type: Expected response format (json, text, or ndjson).
            response_error_values: Values indicating errors in response (optional).
            project_ids: List of project IDs to associate with (optional).

        Returns:
            Created endpoint details including resource_instance_id.

        Example:
            endpoint = client.endpoints.create_custom(
                identifier="custom-llm",
                pentest_url="https://api.example.com/v1/generate",
                headers={"Authorization": "Bearer token123"},
                body={
                    "model": "llama-2-70b",
                    "prompt": "<<PROMPT>>",
                    "max_tokens": 100
                },
                response_jsonpaths=["$.output.text"],
                response_type="json"
            )
        """
        pentest_connection_details = {
            "pentest_url": pentest_url,
            "headers": headers,
            "body": body,
            "method": method,
            "response_type": response_type,
            "response_jsonpaths": response_jsonpaths
        }

        if response_error_values:
            pentest_connection_details["response_error_values"] = response_error_values

        data = {
            "resource_type": "CustomLlmEndpoint",
            "display_name": display_name or f"Custom Endpoint ({identifier})",
            "resource_data": {
                "endpoint_identifier": identifier,
                "pentest_connection_details": pentest_connection_details
            }
        }

        if project_ids:
            data["project_ids"] = project_ids

        endpoint = f"/v1/inventory/customer/{self.client.customer_id}/resources/llm-endpoint"
        return self.client.make_request(endpoint, method="POST", data=data)

    def list(self) -> Dict[str, Any]:
        """
        List all LLM endpoints for the customer.

        Returns:
            Dictionary containing list of endpoints in 'resources' key.

        Example:
            result = client.endpoints.list()
            for endpoint in result['resources']:
                print(endpoint['resource_display_name'])
        """
        endpoint = (
            f"/v1/inventory/customer/{self.client.customer_id}/resources"
            "?resource_category=llm_endpoint"
        )
        return self.client.make_request(endpoint)

    def get_by_name(self, name: str) -> Optional[Dict[str, Any]]:
        """
        Get an endpoint by its display name.

        Args:
            name: Display name of the endpoint.

        Returns:
            Endpoint details if found, None otherwise.

        Example:
            endpoint = client.endpoints.get_by_name("GPT-4 Production")
            if endpoint:
                print(f"Found: {endpoint['resource_instance_id']}")
        """
        endpoints = self.list()
        for endpoint in endpoints.get("resources", []):
            if endpoint.get("resource_display_name") == name:
                return endpoint
        return None
