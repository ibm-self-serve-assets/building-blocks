# Runtime Prompt Evaluations with IBM WatsonX Governance

This repository contains **Runtime Evaluations** for **Prompt Governance** in **Large Language Models (LLMs)** using the **IBM WatsonX Governance SDK**. These evaluations ensure that prompts and responses generated by LLMs are compliant, transparent, and fair during their deployment in real-time.

## Technology Stack

- **Python 3.10+**
- **IBM watsonx.governance SDK**: for model governance, prompt evaluations, and monitoring.
- **IBM watsonx openscale SDK**: for model continuous monitoring and evaluation.
- **Jupyter Notebooks**: for interactive development and experimentation.
- **python-dotenv**: for managing environment configurations.
- **IBM Cloud SDK**: for authentication with IBM Cloud services.

## Prerequisites

Before you can start working with Runtime Evaluations for Prompt Governance, make sure you have the following:

### 1. **IBM WatsonX Governance Service Instance**
   - Create an instance of the **IBM WatsonX Governance** service via the [IBM Cloud Catalog](https://cloud.ibm.com/catalog) to access the necessary tools and APIs for prompt governance.

### 2. **IBM Cloud API Key**
   - Generate an **API Key** for authenticating with IBM Cloud using the [IBM Cloud API Key Generator](https://cloud.ibm.com/docs/account?topic=account-userapikey).

### 3. **Access to IBM WatsonX Governance Service**
   - Ensure you have appropriate access to the **watsonx.governance** service instance to monitor prompts, track model performance, and run evaluations.

---

