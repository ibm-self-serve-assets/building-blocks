# Runtime Monitoring


Monitoring AI models after deployment is essential to ensuring they behave safely and reliably in real-world conditions. Runtime monitoring generally consists of two key AI evaluation categories. 

**(1) AI Guardrails:** 

The first focuses on maintaining control over unwanted or unsafe model behavior, such as detecting and blocking jailbreak attempts, harmful outputs, or other forms of misuse, by implementing robust AI guardrails. 

**(2) Continuous Monitoring:** 

The second focuses on tracking the modelâ€™s performance over time to detect issues like performance drift, which may arise as user input patterns, data distributions, or usage contexts evolve. Together, these continuous monitoring practices help teams identify deviations early, preserve model quality, and maintain trust in AI-driven systems.
  

<img src="real-time-guardrails/images/Guardrails vs monitoring.png" 
     alt="Guardrails vs monitoring" 
     style="width: 80%;"/>
