{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb9e3672",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e6f7ff; padding:10px; border-radius:6px;\">\n",
    "\n",
    "# Design-Time Evaluation of Tool Calling in a LangGraph Agent Using the IBM watsonx.governance Python SDK\n",
    "\n",
    "This notebook demonstrates how to use the Tool call Syntactic Accuracy evaluator from IBM watsonx.governance for governing your applications right in your development environment.\n",
    "\n",
    "\n",
    "First, we will create a question answering agent that is equipped with two custom tools, **\"convert_currency\"** and **\"assess_loan_risk\"** to respond to the user queries. Given the user‚Äôs query, an LLM routes it to the relevant tool. If there is not a relevant tool to answer that question, the agent will generate without a tool. We will use the Agentic AI evaluators from IBM watsonx.governance Python SDK to evaluate the tool calling functionality of the agent in this lab on metrics such as:\n",
    "\n",
    "- Tool call accuracy\n",
    "- Tool call relevance\n",
    "- Tool call latency\n",
    "\n",
    "# Agent Architecture\n",
    "\n",
    "<div>\n",
    "  <img \n",
    "    src=\"https://raw.githubusercontent.com/ibm-self-serve-assets/building-blocks/main/trusted-ai/design-time-evaluations/agents-evaluations/images/Tool Calling_Agent.png\" \n",
    "    alt=\"Advanced Agent\" \n",
    "    width=\"15%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138aaa30",
   "metadata": {},
   "source": [
    "### Install the dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8508dd16",
   "metadata": {},
   "source": [
    "### Note: \n",
    "\n",
    "Ignore the dependency error warning after running the cell below. Your notebook will still run without porblem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0690a5cf-56e2-4a78-b90f-7d9f35700851",
   "metadata": {
    "id": "0690a5cf-56e2-4a78-b90f-7d9f35700851"
   },
   "outputs": [],
   "source": [
    "# Install Watsonx + LangChain stack\n",
    "!pip install --quiet \\\n",
    "    \"ibm-watsonx-gov[agentic,visualization]\" \\\n",
    "    \"ibm-watsonx-ai>=1.3.1,<1.4.0\" \\\n",
    "    \"langchain-ibm>=0.3.10,<0.4.0\" \\\n",
    "    \"langchain-community<=0.3.3\"\n",
    "\n",
    "# Re-pin conflicting dependencies\n",
    "!pip install --quiet --force-reinstall --no-deps \\\n",
    "    \"protobuf==4.21.12\" \\\n",
    "    \"scikit-learn==1.3.2\" \\\n",
    "    \"jsonschema<=4.20.0\" \\\n",
    "    \"grpcio<=1.67.1\"\n",
    "\n",
    "!pip install --quiet ibm_agent_analytics==0.5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f3ec01",
   "metadata": {},
   "source": [
    "**Note**: If you encounter any Torch-related attribute errors while setting up the evaluator, try resolving them by running the cell below to uninstall Torch. This step is required when running in Watson Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8d83bd",
   "metadata": {
    "id": "569230f6-ffad-4302-994b-12bd8e254e1b"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y -qqq torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699cb130-037d-4c24-9a3e-cd90b82ed71c",
   "metadata": {
    "id": "699cb130-037d-4c24-9a3e-cd90b82ed71c"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y -qqq transformers\n",
    "!pip install -qqq \"transformers[tf]<4.38\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c216e",
   "metadata": {},
   "source": [
    "### üîë Configure Authentication\n",
    "\n",
    "\n",
    "Below is a brief description of the required environment variables.  \n",
    "For detailed instructions on how to obtain them, please see the step-by-step PDF guide.  \n",
    "\n",
    "- Only **WATSONX_APIKEY** and **WATSONX_PROJECT_ID** are required to run this notebook.  \n",
    "\n",
    "### First-time setup\n",
    "When you run the code snippet for the first time:  \n",
    "1. A pop-up input bar will appear asking for each variable.  \n",
    "2. Paste your **API key** and press **Enter**.  \n",
    "3. Next, you will be prompted for the **Project ID**. Paste it and press **Enter**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc0a37a-e56f-419a-8bcd-bec828a007a6",
   "metadata": {
    "id": "49023ab5-15a0-470e-9e10-eb1ab69ca007"
   },
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "# For watsonx.governance Cloud\n",
    "_set_env(\"WATSONX_APIKEY\")\n",
    "# _set_env(\"WATSONX_REGION\")\n",
    "# _set_env(\"WXG_SERVICE_INSTANCE_ID\")\n",
    "\n",
    "# set project ID for experiment tracking\n",
    "_set_env(\"WATSONX_PROJECT_ID\")\n",
    "\n",
    "print(\"‚úÖ Environment configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37545253",
   "metadata": {},
   "source": [
    "### Set up the Watsonx LLM model\n",
    "In this section, we initialize the ChatWatsonx from the langchain_ibm package to use IBM's Granite language model (granite-3-2b-instruct) for text generation.\n",
    "Environment variables are used to securely access the IBM Watsonx platform:\n",
    "\n",
    "- WATSONX_URL: Base URL for the Watsonx service.\n",
    "- WATSONX_APIKEY: API key for authentication.\n",
    "- WATSONX_PROJECT_ID: ID of the Watsonx project to scope resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d0b8c5-316e-49a6-bda6-806d51cc1300",
   "metadata": {
    "id": "5b749635-b0bb-4f8d-aea0-7cb3f02cbf70"
   },
   "outputs": [],
   "source": [
    "from langchain_ibm import ChatWatsonx\n",
    "import os\n",
    "\n",
    "llm = ChatWatsonx(\n",
    "    model_id=\"ibm/granite-3-3-8b-instruct\",\n",
    "    url=os.getenv(\"WATSONX_URL\", \"https://us-south.ml.cloud.ibm.com\"),\n",
    "    # Uncomment the following lines if using watsonx.governance on-prem\n",
    "    # username=os.getenv(\"WATSONX_USERNAME\"),\n",
    "    # password=os.getenv(\"WATSONX_PASSWORD\"), # Only one of api_key or password is needed\n",
    "    # instance_id=\"openshift\",\n",
    "    # version=os.getenv(\"WATSONX_VERSION\"),\n",
    "    apikey=os.getenv(\"WATSONX_APIKEY\"),\n",
    "    project_id=os.getenv(\"WATSONX_PROJECT_ID\"),\n",
    "    params={\n",
    "        \"decoding_method\": \"greedy\",\n",
    "        \"temperature\": 0,\n",
    "        \"min_new_tokens\": 5,\n",
    "        \"max_new_tokens\": 250,\n",
    "        \"stop_sequences\": [\"Human:\", \"Observation:\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9711888d-f03b-4bb2-bf62-bc626d896d0d",
   "metadata": {},
   "source": [
    "### Defining Tools for the Agent\n",
    "This cell sets up custom tools that the agent can call during execution. Each tool wraps a specific Python function and provides a structured interface for interaction within the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d326da8-2ed6-4a74-ad19-dabce971bc33",
   "metadata": {
    "id": "9cab72e6-7bb2-41e3-be36-efae68284b9c"
   },
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from enum import Enum\n",
    "import random\n",
    "\n",
    "\n",
    "def convert_currency(amount: float, from_currency: str, to_currency: str) -> dict:\n",
    "    \"\"\"Converts an amount of money from one currency to another (mock rates).\n",
    "\n",
    "    Args:\n",
    "        amount: The amount of money to convert.\n",
    "        from_currency: The currency code of the amount (e.g., \"USD\").\n",
    "        to_currency: The target currency code (e.g., \"EUR\").\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the converted value and details.\n",
    "    \"\"\"\n",
    "    # Mock exchange rates (for demo purposes)\n",
    "    rates = {\n",
    "        \"USD\": {\"EUR\": 0.92, \"JPY\": 148.3, \"GBP\": 0.79},\n",
    "        \"EUR\": {\"USD\": 1.09, \"JPY\": 161.2, \"GBP\": 0.86},\n",
    "        \"JPY\": {\"USD\": 0.0067, \"EUR\": 0.0062, \"GBP\": 0.0053},\n",
    "    }\n",
    "\n",
    "    if from_currency not in rates or to_currency not in rates[from_currency]:\n",
    "        return {\n",
    "            \"error\": f\"Conversion from {from_currency} to {to_currency} is not supported.\"\n",
    "        }\n",
    "\n",
    "    converted = round(amount * rates[from_currency][to_currency], 2)\n",
    "\n",
    "    return {\n",
    "        \"from\": f\"{amount} {from_currency}\",\n",
    "        \"to\": f\"{converted} {to_currency}\",\n",
    "        \"rate_used\": rates[from_currency][to_currency],\n",
    "    }\n",
    "\n",
    "\n",
    "@tool\n",
    "def assess_loan_risk(credit_score: int, income: float, loan_amount: float) -> dict:\n",
    "    \"\"\"Assesses loan application risk based on credit score, income, and loan amount.\n",
    "\n",
    "    Args:\n",
    "        credit_score: Applicant‚Äôs credit score (300‚Äì850).\n",
    "        income: Applicant‚Äôs annual income in USD.\n",
    "        loan_amount: Requested loan amount in USD.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with risk classification, debt-to-income ratio, and recommendation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Debt-to-Income ratio\n",
    "    dti = loan_amount / max(income, 1)  # avoid division by zero\n",
    "    dti_percent = round(dti * 100, 2)\n",
    "\n",
    "    # Rule-based risk classification\n",
    "    if credit_score >= 750 and dti < 0.3:\n",
    "        risk = \"Low\"\n",
    "        recommendation = \"Approve\"\n",
    "    elif credit_score >= 650 and dti < 0.5:\n",
    "        risk = \"Medium\"\n",
    "        recommendation = \"Review manually\"\n",
    "    else:\n",
    "        risk = \"High\"\n",
    "        recommendation = \"Reject\"\n",
    "\n",
    "    return {\n",
    "        \"credit_score\": credit_score,\n",
    "        \"income_usd\": income,\n",
    "        \"loan_amount_usd\": loan_amount,\n",
    "        \"debt_to_income_ratio_percent\": dti_percent,\n",
    "        \"risk_level\": risk,\n",
    "        \"recommendation\": recommendation,\n",
    "    }\n",
    "\n",
    "\n",
    "tools = [convert_currency, assess_loan_risk]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c794232-b5b1-4d9d-a110-c30c0d7e7f98",
   "metadata": {},
   "source": [
    "### Binding the Language Model with Tool Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f4483d-8f04-4bfb-a568-1925311f0981",
   "metadata": {
    "id": "507915d9-fd2f-4ff3-b80c-8addc2a467db"
   },
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b1a6a6",
   "metadata": {},
   "source": [
    "### Set up the State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6627b4d5-c113-4148-9208-bc01acc850b5",
   "metadata": {
    "id": "ece2fb7a-9bc1-478c-961f-6a2ab991bc22"
   },
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        input_text (str):\n",
    "            The user's raw input query or question.\n",
    "        record_id (Optional[str]):\n",
    "            Unique identifier for the record.\n",
    "        generated_text (Optional[str]):\n",
    "            The final output generated by the LLM after processing all contexts.\n",
    "        tool_calls (list):\n",
    "            The list of tools invoked for a user query\n",
    "        messages (list):\n",
    "            List of messages required for the LLM\n",
    "    \"\"\"\n",
    "\n",
    "    messages: list  # List of messages required for the LLM\n",
    "    input_text: str  # The user's raw input query or question\n",
    "    tool_calls: list  # The list of tools invoked for a user query\n",
    "    record_id: str  # Unique identifier for the record.\n",
    "    generated_text: str  # The final output generated by the LLM after processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b709e7c-8663-475e-8e63-75dd7333f35f",
   "metadata": {},
   "source": [
    "#### Initialise the evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e199387c-78ef-4892-be4a-26bd8170011f",
   "metadata": {
    "id": "e199387c-78ef-4892-be4a-26bd8170011f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip uninstall -y -qqq transformers\n",
    "# !pip install -qqq \"transformers[tf]<4.38\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a77379",
   "metadata": {
    "id": "91b17e74-c5d8-4fd9-893a-223093eeafcd"
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_gov.evaluators.agentic_evaluator import AgenticEvaluator\n",
    "\n",
    "evaluator = AgenticEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58990bb1-67c5-469b-833b-45fecef677fc",
   "metadata": {},
   "source": [
    "### Set up LLM Judge for metrics evaluation\n",
    "\n",
    "To evaluate the Tool Call Relevance Metric and Tool Call Parameter Accuracy Metric, an LLM must be used as the judge. To enable this, you need to specify the model provider, model name, and provide the necessary credentials.\n",
    "\n",
    "To use LLM judge to evaluate a metric you need to add the details of the `llm_judge` when creating the metric object. For example:\n",
    "\n",
    "```python\n",
    "# Define LLM Judge using watsonx.ai\n",
    "llm_judge = LLMJudge(\n",
    "    model=WxAIFoundationModel(\n",
    "        model_id=\"meta-llama/llama-3-3-70b-instruct\",\n",
    "        project_id=\"<PROJECT_ID>\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Defining LLM Judge using OpenAI\n",
    "llm_judge = LLMJudge(\n",
    "    model=OpenAIFoundationModel(\n",
    "        model_id=\"gpt-4o-mini\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Specify the LLM judge when initializing the metric\n",
    "@evaluator.evaluate_tool_call_parameter_accuracy(\n",
    "    configuration=AgenticAIConfiguration(**tool_call_metric_config),\n",
    "    metrics=[ToolCallParameterAccuracyMetric(llm_judge=llm_judge)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad96adeb",
   "metadata": {},
   "source": [
    "### Build your langgraph application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9baafc5",
   "metadata": {},
   "source": [
    "#### Define LLM Agent Node\n",
    "\n",
    "Define the logic for invoking the agent within a LangGraph node. It takes the current GraphState and RunnableConfig as input. It invokes the `llm_with_tools ` with the user's input text.\n",
    "\n",
    "Extracts the tools used during the agent's reasoning process.\n",
    "Formats the tool usage into a structured list for further analysis or visualization.\n",
    "\n",
    "Returns the list of tools used, the raw response messages, and a placeholder for generated text.\n",
    "\n",
    "The `llm_agent` node defined below is decorated with IBM watsonx.governance evaluators: `evaluate_tool_call_accuracy` to measure the syntactic correctness of the tool call, `evaluate_tool_call_relevance` to determine whether the tool call made by the LLM agent correctly addresses the user‚Äôs immediate request as the appropriate next step in the conversation, and `evaluate_tool_call_parameter_accuracy` to assess whether all parameter values in a tool call are directly supported by the conversation history or the tool specifications. The accuracy score ranges from `0` to `1`, with values closer to `1` indicating higher accuracy and values closer to `0` indicating lower accuracy. This node reads the user query from the `input_text` attribute from the application state and writes the response into the `tool_calls` attribute and the AIMessage response to the `messages` and set back to the application state.\n",
    "\n",
    "User can specify the evaluators to be computed after the graph invocation by specifying flag `compute_real_time` set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f0d05-5462-489b-82b1-a3f0c3f0948a",
   "metadata": {
    "id": "198fcc1b-000c-40dc-ba1c-774876afa8b8"
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_gov.config import AgenticAIConfiguration\n",
    "from langgraph.config import RunnableConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f035efb-c36b-42a9-bd98-1125d4c844ed",
   "metadata": {
    "id": "97ab6b20-8514-4e63-9afa-14b2131b7434"
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_gov.entities.llm_judge import LLMJudge\n",
    "from ibm_watsonx_gov.entities.foundation_model import WxAIFoundationModel\n",
    "\n",
    "llm_judge = LLMJudge(\n",
    "    model=WxAIFoundationModel(\n",
    "        model_id=\"meta-llama/llama-3-3-70b-instruct\",\n",
    "        project_id=os.getenv(\"WATSONX_PROJECT_ID\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9ebe9-b497-4308-97da-8a9efbd9776b",
   "metadata": {
    "id": "7c47997f-770b-4bd1-a6cd-ce445a4d19c8"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from ibm_watsonx_gov.metrics import (\n",
    "    ToolCallRelevanceMetric,\n",
    "    ToolCallParameterAccuracyMetric,\n",
    ")\n",
    "\n",
    "tool_call_metric_config = {\n",
    "    \"question_field\": \"input_text\",\n",
    "    \"tool_calls_field\": \"tool_calls\",\n",
    "    \"tools\": tools,\n",
    "}\n",
    "\n",
    "\n",
    "@evaluator.evaluate_tool_call_relevance(\n",
    "    configuration=AgenticAIConfiguration(**tool_call_metric_config),\n",
    "    metrics=[ToolCallRelevanceMetric(llm_judge=llm_judge)],\n",
    "    compute_real_time=False,\n",
    ")\n",
    "@evaluator.evaluate_tool_call_parameter_accuracy(\n",
    "    configuration=AgenticAIConfiguration(**tool_call_metric_config),\n",
    "    metrics=[ToolCallParameterAccuracyMetric(llm_judge=llm_judge)],\n",
    "    compute_real_time=False,\n",
    ")\n",
    "@evaluator.evaluate_tool_call_accuracy(\n",
    "    configuration=AgenticAIConfiguration(**tool_call_metric_config),\n",
    "    compute_real_time=False,\n",
    ")\n",
    "def llm_agent(state: GraphState, config: RunnableConfig):\n",
    "\n",
    "    user_query = state[\"input_text\"]\n",
    "    response = llm_with_tools.invoke([HumanMessage(user_query)])\n",
    "\n",
    "    return {\"messages\": [response], \"tool_calls\": response}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dbcdc2-4dcc-4a40-bc46-8002d73f835c",
   "metadata": {},
   "source": [
    "### Tool Condition Function\n",
    "- The tools_condition function evaluates the agent's response and determines the next step in the LangGraph workflow based on whether the LLM agent's most recent response includes a tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3216409f-c9c2-4c75-97f9-79f64bc4c022",
   "metadata": {
    "id": "3248e110-42e2-45e6-bc09-68fb940dcc78"
   },
   "outputs": [],
   "source": [
    "def tools_condition(state: GraphState, config: RunnableConfig) -> str:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    # If the agent wants to call a tool, go to tools node\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        return \"Has Tools\"\n",
    "    # Otherwise, we're done\n",
    "    else:\n",
    "        return \"No Tools\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d038d5-809b-41c2-b296-b888294313d7",
   "metadata": {},
   "source": [
    "### Define Answer generation node\n",
    "This node generates the final answer based on the tool execution results, if available; otherwise, it returns a default response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551f3f7-bc8c-409a-901a-8c604da107d2",
   "metadata": {
    "id": "6dff0523-bb12-4232-8a03-0218d53458e8"
   },
   "outputs": [],
   "source": [
    "def generate_response(state: GraphState, config: RunnableConfig) -> dict:\n",
    "    tool_results = \"\\n\".join(\n",
    "        [msg.content for msg in state[\"messages\"] if isinstance(msg, ToolMessage)]\n",
    "    )\n",
    "    print(\"\\n########## tool results: \", tool_results)\n",
    "    if not tool_results:\n",
    "        tool_results = \"I'm sorry, but the agent couldn't process your query. Please try rephrasing or providing more details so I can better assist you.\"\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"Final Answer:\\n{tool_results}\")],\n",
    "        \"generated_text\": tool_results,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e103cff",
   "metadata": {},
   "source": [
    "#### Assemble your application\n",
    "\n",
    "**Note:** If you encounter an issue while importing ToolNode from langgraph, please force install langgraph using the command `pip install --upgrade --force-reinstall \"langgraph>=0.3.34,<0.4.0\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee9563d-6ac0-4d42-b9a5-6a796d8a9aff",
   "metadata": {
    "id": "fee9563d-6ac0-4d42-b9a5-6a796d8a9aff"
   },
   "outputs": [],
   "source": [
    "!pip install -qqq --upgrade --force-reinstall \"langgraph>=0.3.34,<0.4.0\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c637f-7ebd-425d-95a0-717cb1c5f810",
   "metadata": {
    "id": "9646f58e-9018-49e1-be82-a4da285e314f"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "\n",
    "# from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "\n",
    "\n",
    "graph = StateGraph(GraphState)\n",
    "\n",
    "graph.add_node(\"LLM Agent\", llm_agent)\n",
    "graph.add_node(\"Generate Response\", generate_response)\n",
    "graph.add_node(\"Tools\", ToolNode(tools))\n",
    "\n",
    "graph.set_entry_point(\"LLM Agent\")\n",
    "graph.add_conditional_edges(\n",
    "    \"LLM Agent\",\n",
    "    tools_condition,\n",
    "    {\"Has Tools\": \"Tools\", \"No Tools\": \"Generate Response\"},\n",
    ")\n",
    "graph.add_edge(\"Tools\", \"Generate Response\")\n",
    "graph.add_edge(\"Generate Response\", END)\n",
    "\n",
    "# Compile the graph\n",
    "rag_app = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac09817-a894-4478-9ddf-08cc57365b9b",
   "metadata": {},
   "source": [
    "#### Display the graph\n",
    "\n",
    "**Note:** you can get the link from below and paste it to https://mermaid.live to see the graph structure. To see the graph image, follow these steps: \n",
    "\n",
    "- Copy the entire printed text.\n",
    "\n",
    "- Open https://mermaid.live\n",
    "\n",
    "- Paste it in the editor.\n",
    "\n",
    "The diagram will render instantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d10de4-85ac-47eb-ba43-15d67216ac50",
   "metadata": {
    "id": "b1d10de4-85ac-47eb-ba43-15d67216ac50"
   },
   "outputs": [],
   "source": [
    "# # Get the raw Mermaid graph syntax\n",
    "# mermaid_code = rag_app.get_graph().draw_mermaid()\n",
    "\n",
    "# # Print it so you can copy-paste into mermaid.ink or mermaid.live\n",
    "# print(mermaid_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d7726c",
   "metadata": {},
   "source": [
    " <div style=\"background-color:#dff6dd; padding:10px; border-radius:6px;\">\n",
    "  <h3 style=\"margin:0;\">\n",
    "  \n",
    "  ü§î Discussion Point: \n",
    "\n",
    "Think of an Agent equipped with multiple tools, how should the agent decide which tool to invoke?   \n",
    "\n",
    "What criteria would you use (e.g., accuracy, latency, reliability, interpretability)?\n",
    "\n",
    "How might you design the graph to handle fallbacks or errors if the first tool call fails?\n",
    "\n",
    "Should the agent always try to reason first, or immediately delegate to a tool? Why might reasoning first not be beneficial?\n",
    "  </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f1b09-f779-4596-aff2-6e4250a6a740",
   "metadata": {},
   "source": [
    "### Do a single invocation\n",
    "\n",
    "Now the application is invoked for a single row of data.\n",
    "\n",
    "#### Evaluator Integration\n",
    "1. `evaluator.start_run()`: Begins tracking this specific invocation. \n",
    "\n",
    "2. `evaluator.end_run()`: Marks the end of the evaluation run. All metrics will now be captured and logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74125f1-503f-4aca-8346-5dc564ee3f25",
   "metadata": {
    "id": "cf051b60-ba76-4051-9067-8ed58cb37223"
   },
   "outputs": [],
   "source": [
    "evaluator.start_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e901f98-03da-41d3-a23b-56722bfbe2bd",
   "metadata": {
    "id": "aa567ba7-b9c8-4992-b241-8c90290a0932"
   },
   "outputs": [],
   "source": [
    "result = rag_app.invoke(\n",
    "    {\n",
    "        \"input_text\": \"what is the risk score for a 30000 loan with credit score 780 and income 120000?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79164fb9-9ee0-4738-9713-b370e00088b6",
   "metadata": {
    "id": "48e340e5-e7ff-4b21-9ed4-f03dc367af35"
   },
   "outputs": [],
   "source": [
    "evaluator.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856e0018-3bf7-4f25-a248-28c3df755e06",
   "metadata": {},
   "source": [
    "### Prepare the app results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c3e2b1-3615-41ff-b669-53d509cc8530",
   "metadata": {
    "id": "04032ed4-83ab-4191-822c-50437862ffca"
   },
   "outputs": [],
   "source": [
    "result = evaluator.get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800307bb-fdb1-439e-9ce5-8033b39440c9",
   "metadata": {
    "id": "7c00e749-e559-4dd7-9593-f4780c9365c9"
   },
   "outputs": [],
   "source": [
    "display(result.to_df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4860d09-0431-49f7-a1ba-5465cdf197ec",
   "metadata": {
    "id": "2f3ea049-3d93-4d20-9ba2-f05d3a81e916"
   },
   "outputs": [],
   "source": [
    "result.get_aggregated_metrics_results(node_name=\"LLM Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d6f6be-8049-4c87-8440-c487bafd8de7",
   "metadata": {},
   "source": [
    "### Invoke the graph on multiple rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadd5531-a827-41b3-a2c4-107b9b8be3b1",
   "metadata": {},
   "source": [
    "IBM watsonx.governance also supports evaluation of Agentic Applications using batch invocation. The following DataFrame contains sample questions that will be used to demonstrate this capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a3983-0dd9-4601-989a-16d9791e6f05",
   "metadata": {
    "id": "d5e5ed6a-4568-42d3-88e8-d2f09f6cc65d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "question_bank_df = pd.read_csv(\n",
    "    \"https://ibm.box.com/shared/static/khgg3aeo157z97yha1f3dfml6lliomqj.csv\"\n",
    ")\n",
    "\n",
    "question_bank_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9876d6-472b-4ced-b82a-7a342e8a9d63",
   "metadata": {},
   "source": [
    "#### Execute Batch Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ff0555-8b0b-4243-b3d1-770899a858b8",
   "metadata": {
    "id": "e178fc24-169e-4641-91b4-081c2a9ffd16",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluator.start_run()\n",
    "result = rag_app.batch(inputs=question_bank_df.to_dict(\"records\"))\n",
    "evaluator.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2439f788-cc21-4c64-ac6b-9b4d28e55e8c",
   "metadata": {
    "id": "716656b8-259c-4f7e-aabb-66138c08a957"
   },
   "outputs": [],
   "source": [
    "result = evaluator.get_result()\n",
    "display(result.to_df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6696f2-eb8e-4db3-bcda-b55cb6cc75ef",
   "metadata": {
    "id": "bb4bb408-a659-45b1-9821-12b0156f374d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.get_aggregated_metrics_results(node_name=\"LLM Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fda386d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e6f7ff; padding:20px; border-radius:10px;\n",
    "            border: 2px solid #3399ff; text-align:left; \n",
    "            display:inline-block;\">\n",
    "\n",
    "  <h1 style=\"margin-top:0;\">üéâ üèÜ ü•≥ Congratulations!</h1>\n",
    "\n",
    "  <p style=\"font-size:18px;\">\n",
    "You have completed design-time evaluations of the tool-calling functionality in a LangGraph RAG agent designed to answer questions using custom tools.\n",
    "</p>\n",
    "\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
