{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e6f7ff; padding:10px; border-radius:6px;\">\n",
    "\n",
    "# Design-Time Evaluation of LangGraph-Based RAG Agents with IBM watsonx.governance Python SDK\n",
    "\n",
    "\n",
    "This notebook demonstrates how to use the Agentic AI evaluators from IBM watsonx.governance for governing your agentic applications right in your development environment.\n",
    "\n",
    "First, we will create a LangGraph RAG agent and then use watsonx Agentic AI evaluator to evaluate the agent‚Äôs performance. The agent for this lab has the following architecture. It uses local documents to perform a RAG task. \n",
    "We evaluate this agent on range of metrics:\n",
    "answer similarity\n",
    "\n",
    "- context relevance\n",
    "- faithfulness\n",
    "- retrieval latency\n",
    "- generation latency\n",
    "- interaction cost\n",
    "- interaction duration\n",
    "- input token count\n",
    "- output token count\n",
    "\n",
    "# Agent Architecture\n",
    "\n",
    "<div>\n",
    "  <img \n",
    "    src=\"https://raw.githubusercontent.com/ibm-self-serve-assets/building-blocks/main/trusted-ai/design-time-evaluations/agents-evaluations/images/Basic_Agent.png\" \n",
    "    alt=\"Advanced Agent\" \n",
    "    width=\"10%\">\n",
    "</div>\n",
    "\n",
    "## Important Note:\n",
    "\n",
    "If you are using this watsonx instance for the first time, you need to first associate your instance with a runtime. See the step-by-step PDF guide for detailed instruction.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f183ec33-431b-46e7-8a93-83297e50c00d"
   },
   "outputs": [],
   "source": [
    "# from ibm_agent_analytics.instrumentation.utils import record_span_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the dependencies\n",
    "\n",
    "**Note:** When running the cell below, ignore the error warning for depency mismatch, it won't affect the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6d288b31-a8b2-4a16-8d24-81c89ba06452"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet \"ibm-watsonx-gov[agentic,visualization,metrics]\" \"langchain-chroma<0.3.0\" \"chromadb>=1.0.13,<2.0.0\" \"langchain-openai<=0.3.0\"\n",
    "!pip install --quiet ibm_agent_analytics==0.5.4\n",
    "!pip uninstall -y -qqq torch\n",
    "!pip uninstall --quiet protobuf -y\n",
    "!pip install --quiet --no-deps protobuf==4.25.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîë Configure Authentication\n",
    "\n",
    "\n",
    "Below is a brief description of the required environment variables.  \n",
    "For detailed instructions on how to obtain them, please see the step-by-step PDF guide.  \n",
    "\n",
    "- Only **WATSONX_APIKEY** and **WATSONX_PROJECT_ID** are required to run this notebook.  \n",
    "\n",
    "### First-time setup\n",
    "When you run the code snippet for the first time:  \n",
    "1. A pop-up input bar will appear asking for each variable.  \n",
    "2. Paste your **API key** and press **Enter**.  \n",
    "3. Next, you will be prompted for the **Project ID**. Paste it and press **Enter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5d7aee1-3b16-4380-9f5f-c4ae99bd52fb"
   },
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"WATSONX_APIKEY\")\n",
    "_set_env(\"WATSONX_PROJECT_ID\")\n",
    "\n",
    "print(\"‚úÖ Environment configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Vector Store using WX embedding\n",
    "\n",
    "We selected a few medium posts by [Manish Bhide](https://medium.com/@manish.bhide) and [Ravi Chamarthy](https://medium.com/@ravi-chamarthy) that focus on the various capabilities in IBM watsonx.governance (and erstwhile IBM Watson OpenScale). Hence, our RAG queries will focus on these capabilities covered in the above posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a75ad5e-85aa-47eb-9f13-3031d05c7e79"
   },
   "source": [
    "Downloads a JSON file containing the medium posts from a shared URL and saves it locally as medium_articles.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80c78f06-282d-4dad-b898-93cde000b17a"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://ibm.box.com/shared/static/o6jp3gfl3smyegjmjvsri1ll6zpq5jcv.json\"\n",
    "r = requests.get(url)\n",
    "r.raise_for_status()\n",
    "\n",
    "with open(\"medium_articles.json\", \"wb\") as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Medium articles from JSON, embeds them with Watsonx embedding model, and stores them in a Chroma vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16b6f926-ab22-41b9-8d73-5cfa9d339cf1"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.vectorstores import (\n",
    "    Chroma,\n",
    ")  # Chroma: a vector store for saving/retrieving embeddings.\n",
    "from langchain.schema import (\n",
    "    Document,\n",
    ")  # Document: LangChain schema wrapper for text + metadata\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "\n",
    "# Load JSON\n",
    "with open(\"medium_articles.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract valid documents\n",
    "docs = []\n",
    "for i, item in enumerate(data):\n",
    "    if \"id\" in item and \"document\" in item and item[\"document\"].strip():\n",
    "        docs.append(\n",
    "            Document(page_content=item[\"document\"], metadata={\"id\": str(item[\"id\"])})\n",
    "        )\n",
    "    else:\n",
    "        print(f\":warning: Skipping entry {i}: missing 'id' or 'document'\")\n",
    "\n",
    "# create an embedding model instance\n",
    "embedding_model = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/slate-30m-english-rtrvr\",\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    apikey=os.environ[\"WATSONX_APIKEY\"],\n",
    "    project_id=os.environ[\"WATSONX_PROJECT_ID\"],\n",
    ")\n",
    "\n",
    "# Persist directory for Chroma (save vecor database locally)\n",
    "persist_dir = \"vector_store\"\n",
    "\n",
    "# Create Chroma vector store\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=docs, embedding=embedding_model, persist_directory=persist_dir\n",
    ")\n",
    "\n",
    "# Save to disk\n",
    "vector_store.persist()\n",
    "\n",
    "# Optional: create retriever\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Test retrieval\n",
    "results = retriever.get_relevant_documents(\"example query text\")\n",
    "for r in results:\n",
    "    print(\":mag:\", r.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the State\n",
    "\n",
    "The `ibm-watsonx-gov` library provides a pydantic based state class - `EvaluationState`. This provides various attributes for your use e.g. `input_text` is for storing the application input, `context` is for storing the context documents. For simple applications, developers can extend this class for their use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fb01112b-ebab-4cc6-9a5d-1e789100b087"
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_gov.entities.state import EvaluationState\n",
    "\n",
    "\n",
    "class AppState(EvaluationState):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the evaluator\n",
    "\n",
    "For evaluating your Agentic AI applications, you need to first instantiate the `AgenticEvaluator` class. This class defines a few evaluators to compute the different metrics.\n",
    "\n",
    "We are going to use the following evaluators in this notebook:\n",
    "1. `evaluate_context_relevance` : To compute context relevance metric of your content retrieval tool.\n",
    "2. `evaluate_faithfulness`: To compute faithfulness metric of your answer generation tool. This metric does not require ground truth.\n",
    "3. `evaluate_answer_similarity`: To compute answer similarity metric of your answer generation tool. This metric requires ground truth for computation.\n",
    "\n",
    "You can specify the evaluators to be computed after the graph invocation by specifying flag `compute_real_time` set to False (eg: `evaluate_context_relevance(compute_real_time=False)`)\n",
    "\n",
    "Check this documentation for a comprehensive set of evaluation metircs in watsonx.governance: https://www.ibm.com/docs/en/watsonx/w-and-w/2.2.0?topic=models-evaluation-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3ace6d0-a98f-4441-adf1-b67f327a916e"
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_gov.evaluators.agentic_evaluator import AgenticEvaluator\n",
    "\n",
    "evaluator = AgenticEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your LangGraph Agentic Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bda0fcb6-07d2-464a-88a1-2ee813ca076e"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.config import RunnableConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define content retrieval node\n",
    "\n",
    "We are using a Similarity with Threshold Retrieval strategy. This will fetch the top 3 documents matching the query if the threshold score is more than 0.1\n",
    "\n",
    "The `retrieval_node` tool defined below is decorated with IBM watsonx.governance evaluator `evaluate_context_relevance` tool to compute the context relevance metric. This node reads the user query from the `input_text` attribute from the application state and writes the result into the `context` attribute back to the application state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6f3b305-351b-476f-868a-fe12529ddc58"
   },
   "outputs": [],
   "source": [
    "@evaluator.evaluate_context_relevance(compute_real_time=False)\n",
    "def retrieval_node(state: AppState, config: RunnableConfig) -> dict:\n",
    "    similarity_threshold_retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={\"k\": 3, \"score_threshold\": 0.1},\n",
    "    )\n",
    "    context = similarity_threshold_retriever.invoke(state.input_text)\n",
    "\n",
    "    return {\"context\": [doc.page_content for doc in context]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define answer generation tool\n",
    "\n",
    "We are using `llama-3-3-70b-instruct` to generate an answer for our query.\n",
    "\n",
    "The `generate_node` defined below is decorated with two evaluators `evaluate_faithfulness` and `evaluate_answer_similarity` for computing answer quality metrics. Like in the previous cell, this node reads the user query from the `input_text` attribute from the application state, the `context` attribute consists of the context chunks. After generating the answer, the node writes the result into the `generated_text` attribute back to the application state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ff15aab-31ff-4607-9f8d-a16f0f61e136"
   },
   "outputs": [],
   "source": [
    "from langchain_ibm import WatsonxLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e211bab8-c7bb-4450-9246-5f3cf7ce9c13"
   },
   "outputs": [],
   "source": [
    "@evaluator.evaluate_faithfulness(compute_real_time=False)\n",
    "@evaluator.evaluate_answer_similarity(compute_real_time=False)\n",
    "def generate_node(state: AppState, config: RunnableConfig) -> dict:\n",
    "    generate_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Answer the following question based on the given context:\\n\"\n",
    "        \"Context: {context}\\n\"\n",
    "        \"Question: {input_text}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    formatted_prompt = generate_prompt.invoke(\n",
    "        {\"input_text\": state.input_text, \"context\": \"\\n\".join(state.context)}\n",
    "    )\n",
    "\n",
    "    # Initialize WatsonX LLM\n",
    "    llm = WatsonxLLM(\n",
    "        model_id=\"meta-llama/llama-3-3-70b-instruct\",\n",
    "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        project_id=os.getenv(\"WATSONX_PROJECT_ID\"),\n",
    "        params={\n",
    "            \"max_new_tokens\": 500,\n",
    "            \"decoding_method\": \"greedy\",\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"stop_sequences\": [\".\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    result = llm.invoke(formatted_prompt)\n",
    "\n",
    "    # Normalize result\n",
    "    if hasattr(result, \"content\"):  # AIMessage\n",
    "        output_text = result.content\n",
    "    elif isinstance(result, str):  # plain string\n",
    "        output_text = result\n",
    "    else:\n",
    "        raise TypeError(f\"Unexpected llm.invoke return type: {type(result)}\")\n",
    "\n",
    "    return {\"generated_text\": output_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble your application\n",
    "\n",
    "Build and compile a LangGraph workflow connecting the retrieval and generation nodes for the RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5527a766-00c9-4850-8638-94b0ea6524a5"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "\n",
    "graph = StateGraph(AppState)\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node(\"Retrieval \\nNode\", retrieval_node)\n",
    "graph.add_node(\"Generation \\nNode\", generate_node)\n",
    "\n",
    "# Add edges\n",
    "graph.add_edge(START, \"Retrieval \\nNode\")\n",
    "graph.add_edge(\"Retrieval \\nNode\", \"Generation \\nNode\")\n",
    "graph.add_edge(\"Generation \\nNode\", END)\n",
    "\n",
    "# Compile the graph\n",
    "rag_app = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9328094-c4c6-4b35-a284-224de9c39ca1"
   },
   "source": [
    "#### Display the graph\n",
    "\n",
    "**Note:** you can get the link from below and paste it to https://mermaid.live to see the graph structure. To see the graph image, follow these steps: \n",
    "\n",
    "- Copy the entire printed text.\n",
    "\n",
    "- Open https://mermaid.live\n",
    "\n",
    "- Paste it in the editor.\n",
    "\n",
    "The diagram will render instantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7aa9e197-19e0-49c2-8b76-788725323758"
   },
   "outputs": [],
   "source": [
    "# # Get the raw Mermaid graph syntax\n",
    "# mermaid_code = rag_app.get_graph().draw_mermaid()\n",
    "\n",
    "# # Print it so you can copy-paste into mermaid.ink or mermaid.live\n",
    "# print(mermaid_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"background-color:#dff6dd; padding:10px; border-radius:6px;\">\n",
    "  <h3 style=\"margin:0;\">\n",
    "  \n",
    "  ü§î Discussion Point: \n",
    "  \n",
    "In this agentic example, what metrics do you think are important for evaluating the agent‚Äôs performance? </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do a single invocation\n",
    "\n",
    "Now the application is invoked for a single row of data. You will see two new keys as the input:\n",
    "1. `ground_truth`: As the name suggests, this attribute holds the ground truth for your input text. This is needed for the answer similarity metric, which is a reference based metric. For the other metrics, this is not required.\n",
    "2. `interaction_id`: This is required so that IBM watsonx.governance can keep track of individual rows and associate metrics with each row. This will become evident when we do batch invocation in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f20fbf20-7f9d-458b-8990-77a5333aa6aa"
   },
   "outputs": [],
   "source": [
    "evaluator.start_run()\n",
    "result = rag_app.invoke(\n",
    "    {\n",
    "        \"input_text\": \"What is counterfactual fairness?\t\",\n",
    "        \"ground_truth\": \"Counterfactual fairness is a fairness criterion used in machine learning to ensure that a model‚Äôs decisions remain unchanged if a protected attribute (e.g., race, gender) were different, while keeping everything else the same.\",\n",
    "        \"interaction_id\": \"1\",\n",
    "    }\n",
    ")\n",
    "evaluator.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aa8dcfe-8518-4c60-a686-55bb4fb04fde"
   },
   "outputs": [],
   "source": [
    "result.keys()\n",
    "for key, val in result.items():\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"{key=}\")\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the app results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab33df26-150c-4192-9759-0a77fbf062e4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_result = evaluator.get_result()\n",
    "metric_result = eval_result.to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the metric results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ce76c05-545d-4a91-9237-234ee1939537"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "display(metric_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Metric Results for a specific node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6498352d-37a1-4b36-b28f-2e1c60cb63bc"
   },
   "outputs": [],
   "source": [
    "eval_result.get_aggregated_metrics_results(node_name=\"Generation \\nNode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the graph on multiple rows\n",
    "\n",
    "IBM watsonx.governance evaluation of Agentic Applications can be done with batch invocation too. Here, a dataframe with questions and ground truths for those questions have been defined. The dataframe index has been used as  `record_id` to uniquely identify each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1b0898f6-00e5-4f50-8714-6113bd527d89"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "question_bank_df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/IBM/ibm-watsonx-gov/refs/heads/samples/notebooks/data/agentic/medium_question_bank.csv\"\n",
    ")\n",
    "\n",
    "question_bank_df[\"interaction_id\"] = question_bank_df.index.astype(str)\n",
    "question_bank_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the metric results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86ab1d5a-169b-4865-b3b6-a49525c9c61e"
   },
   "outputs": [],
   "source": [
    "evaluator.start_run()\n",
    "result = rag_app.batch(inputs=question_bank_df.to_dict(\"records\"))\n",
    "evaluator.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d22bcbaa-b3ee-4c03-9db2-e2ed9037e206"
   },
   "outputs": [],
   "source": [
    "eval_result = evaluator.get_result()\n",
    "metric_result = eval_result.to_df()\n",
    "display(metric_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e6f7ff; padding:20px; border-radius:10px;\n",
    "            border: 2px solid #3399ff; text-align:left; \n",
    "            display:inline-block;\">\n",
    "\n",
    "  <h1 style=\"margin-top:0;\">üéâ üèÜ ü•≥ Congratulations!</h1>\n",
    "\n",
    "  <p style=\"font-size:18px;\">\n",
    "You have completed basic design-time evaluations of a LangGraph RAG agent that answers questions using local documents. \n",
    "</p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "637df7da-9423-4265-a896-c017cd640caa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
