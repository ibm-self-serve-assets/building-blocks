{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cbfa099",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e6f7ff; padding:10px; border-radius:6px;\">\n",
    "\n",
    "# Advanced Evaluation of Langraph Agents with **watsonx governance**\n",
    "\n",
    "This notebook demonstrates advanced evaluation capabilities of IBM watsonx.governance for monitoring and governing production-grade LangGraph agentic systems. These evaluators provide enterprise-ready governance for complex agentic architectures, moving beyond basic LLM assessments.\n",
    "\n",
    "We first create a question answering agent that can use local documents or web search to answer the question. The agent will context relevance to decide what tool. We then use the Agentic AI evaluators from IBM watsonx.governance Python SDK to evaluate this agent on metrics including:\n",
    "- Retrieval context relevance\n",
    "- Web search context relevance\n",
    "- Retrieval precision\n",
    "- Web search precision\n",
    "- PII\n",
    "- HAP\n",
    "- HARM\n",
    "- Jailbreak\n",
    "- Sexual content\n",
    "- Latency\n",
    "- Cost\n",
    "\n",
    "\n",
    "\n",
    "# Agent Architecture\n",
    "\n",
    "<div>\n",
    "  <img \n",
    "    src=\"https://raw.githubusercontent.com/ibm-self-serve-assets/building-blocks/main/trusted-ai/design-time-evaluations/agents-evaluations/images/Advanced_Agent.png\" \n",
    "    alt=\"Advanced Agent\" \n",
    "    width=\"15%\">\n",
    "</div>\n",
    "\n",
    "\n",
    "## Important Note:\n",
    "\n",
    "If you are using this watsonx instance for the first time, you need to first associate your instance with a runtime. See the step-by-step PDF guide for detailed instruction.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db33f3d",
   "metadata": {},
   "source": [
    "## 1. Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1921ce",
   "metadata": {},
   "source": [
    "### Install the dependencies\n",
    "\n",
    "**Note:** Ignore the dependecy errors when running the next two cells. Your code will run without problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc213652-41dd-4685-8ca8-5608f07ba882",
   "metadata": {
    "id": "bc213652-41dd-4685-8ca8-5608f07ba882"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet \"ibm-watsonx-gov[agentic,visualization]\" \"langchain-chroma<0.3.0\" \"chromadb>=1.0.13,<2.0.0\" \"langchain-openai<=0.3.0\"\n",
    "!pip uninstall --quiet -y torch\n",
    "!pip install --quiet ddgs\n",
    "# !pip install --quiet \"ddgs[all]\"\n",
    "!pip install --quiet langchain-ibm\n",
    "!pip install --quiet sentence-transformers\n",
    "!pip install --quiet ibm_agent_analytics==0.5.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e893c24",
   "metadata": {
    "id": "84675769-ecb6-45f3-837a-99d69fb6bc42"
   },
   "outputs": [],
   "source": [
    "!pip uninstall --quiet protobuf -y\n",
    "# !pip install --quiet protobuf==4.25.3\n",
    "!pip install --quiet protobuf==4.25.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f41226",
   "metadata": {},
   "source": [
    "### 🔑 Configure Authentication\n",
    "\n",
    "\n",
    "Below is a brief description of the required environment variables.  \n",
    "For detailed instructions on how to obtain them, please see the step-by-step PDF guide.  \n",
    "\n",
    "- Only **WATSONX_APIKEY** and **WATSONX_PROJECT_ID** are required to run this notebook.  \n",
    "\n",
    "### First-time setup\n",
    "When you run the code snippet for the first time:  \n",
    "1. A pop-up input bar will appear asking for each variable.  \n",
    "2. Paste your **API key** and press **Enter**.  \n",
    "3. Next, you will be prompted for the **Project ID**. Paste it and press **Enter**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b837a0",
   "metadata": {
    "id": "e8c2b172-d82e-4db4-a3f8-b44abb7dd00c"
   },
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "# For watsonx.governance Cloud\n",
    "_set_env(\"WATSONX_APIKEY\")\n",
    "# _set_env(\"WATSONX_REGION\")\n",
    "# _set_env(\"WXG_SERVICE_INSTANCE_ID\")\n",
    "\n",
    "# set project ID for experiment tracking\n",
    "_set_env(\"WATSONX_PROJECT_ID\")\n",
    "\n",
    "print(\"✅ Environment configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be562c",
   "metadata": {},
   "source": [
    "### Set URLs for data\n",
    "First URL is for your RAG PDF source document.\n",
    "\n",
    "Second one is for the set of test questions and ground truth to evaluate the RAG app.\n",
    "\n",
    "Test Question CSV should have two columns: `input_text`, `ground_truth`.\n",
    "\n",
    "`input_text` should be questions for the RAG app, `ground_truth` is examples of \"good\" or expected answers to the RAG query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcd8fb9",
   "metadata": {
    "id": "cff4f9d3-00f2-4418-9414-491861a5a764"
   },
   "outputs": [],
   "source": [
    "RAG_DATA_PDF_URL_1 = (\n",
    "    \"https://ibm.box.com/shared/static/a9o9dsyzwcuwneugha769cnkf931or6f.pdf\"\n",
    ")\n",
    "TEST_QUESTIONS_CSV_URL_1 = (\n",
    "    \"https://ibm.box.com/shared/static/yyjyg4xw5ix6xag3p8wsyqc2cobrz655.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8949120",
   "metadata": {},
   "source": [
    "### Get rid of any persistent files from prior runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aca048",
   "metadata": {
    "id": "bd6d4ccc-d98a-4223-ac58-6260cc77904b"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"vector_store\", ignore_errors=True)\n",
    "!rm -rf ./vector_store\n",
    "%rm loan_doc.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278eb200",
   "metadata": {
    "id": "e0d250ba-c859-498a-9aa8-3d66eccedf07"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "for root, _, files in os.walk(\".\"):\n",
    "    for name in files:\n",
    "        path = os.path.join(root, name)\n",
    "        size = os.path.getsize(path)\n",
    "        print(f\"📄 {path} — {size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1925db4d",
   "metadata": {},
   "source": [
    "## 2. Create RAG vector db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a342af",
   "metadata": {},
   "source": [
    "### Go grab a pdf as a RAG source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5326e3",
   "metadata": {
    "id": "61f39029-6872-451d-92a1-fe3d25ab4f4f"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = RAG_DATA_PDF_URL_1\n",
    "r = requests.get(url)\n",
    "r.raise_for_status()\n",
    "with open(\"loan_doc.pdf\", \"wb\") as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7835bf",
   "metadata": {},
   "source": [
    "### Set up the local vector store\n",
    "\n",
    "Chunk the PDF and store it in the vector DB to be used by the RAG app.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0b5f4",
   "metadata": {
    "id": "29e0145c-156b-42f2-a11a-8906101bca92"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ibm.embeddings import WatsonxEmbeddings\n",
    "\n",
    "# Load PDF\n",
    "loader = PyPDFLoader(\"loan_doc.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Chunk documents to stay under token limit\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400, chunk_overlap=50  # safely below 512-token max\n",
    ")\n",
    "chunked_docs = splitter.split_documents(documents)\n",
    "\n",
    "# Initialize IBM embedding model\n",
    "embedding_model = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/slate-30m-english-rtrvr\",\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    apikey=os.environ[\"WATSONX_APIKEY\"],\n",
    "    project_id=os.environ[\"WATSONX_PROJECT_ID\"],\n",
    ")\n",
    "\n",
    "# Persist directory for Chroma\n",
    "persist_dir = \"vector_store\"\n",
    "\n",
    "# Create Chroma vector store\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunked_docs, embedding=embedding_model, persist_directory=persist_dir\n",
    ")\n",
    "\n",
    "# Save to disk\n",
    "vector_store.persist()\n",
    "\n",
    "# Create retriever\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Test retrieval\n",
    "results = retriever.get_relevant_documents(\"Auto loans versus personal loans\")\n",
    "for r in results:\n",
    "    print(\"🔍\", r.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dabd92",
   "metadata": {},
   "source": [
    "## 3. Create Langraph Agent Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1979074",
   "metadata": {},
   "source": [
    "### Set up the State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04000ffc",
   "metadata": {
    "id": "c9e3217e-3f07-4e10-a162-887052a30713"
   },
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        input_text (str):\n",
    "            The user's raw input query or question.\n",
    "        ground_truth (Optional[str]):\n",
    "            Reference/correct answer (if available). Used for evaluation.\n",
    "        local_context (List[str]):\n",
    "            Context retrieved from local knowledge base or vector store.\n",
    "        web_context (List[str]):\n",
    "            Context fetched from web searches (if used).\n",
    "        generated_text (Optional[str]):\n",
    "            The final output generated by the LLM after processing all contexts.\n",
    "    \"\"\"\n",
    "\n",
    "    input_text: str  # The user's raw input query or question\n",
    "    ground_truth: str  # Reference/correct answer (if available). Used for evaluation\n",
    "    local_context: list[str]  # Context retrieved from vector store\n",
    "    web_context: list[str]  # Context fetched from web searches (if used)\n",
    "    generated_text: (\n",
    "        str  # The final output generated by the LLM after processing all contexts\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81649ad",
   "metadata": {},
   "source": [
    "### Set up the evaluator\n",
    "\n",
    "For evaluating your Agentic AI applications, you need to first instantiate the `AgenticEvaluator` class. This class defines evaluators to compute the different metrics at the node and interaction level.\n",
    "\n",
    "We are going to use the following evaluator group in this notebook:\n",
    "1. `evaluate_retrieval_quality` : To compute retrieval quality metrics on your content retrieval tool. Retrieval Quality metrics include Context Relevance, Retrieval Precision, Average Precision, Hit Rate, Reciprocal Rank, NDCG\n",
    "2. `evaluate_answer_quality`: To compute answer quality metric of your answer generation tool. Answer Quality metrics include Answer Relevance, Faithfulness, Answer Similarity, Unsuccessful Requests. Answer Similarity metric requires ground truth. \n",
    "\n",
    "#### Configuring Evaluations\n",
    "\n",
    "You can define the `AgenticApp` class to define the interaction, node metrics to be computed.\n",
    "\n",
    "You can define the metrics to be computed at the agentic app(interaction) level in metrics_configuration in `AgenticApp`.\n",
    "You can specify the node level metrics to be computed along with the graph invocation using the decorators on the function.\n",
    "You can specify the node level metrics to be computed after with the graph invocation using the decorators and with flag `compute_real_time` set to False or specify in the `nodes` attribute in `AgenticApp`, but it's recommended to use only one of these approaches to avoid conflicts or redundancy.\n",
    "\n",
    "You can define evaluation configurations by specifying relevant fields for different evaluation types, such as retrieval quality and answer quality.\n",
    "\n",
    "For example, a configuration for evaluating retrieval quality might look like this:  \n",
    "\n",
    "```python\n",
    "retrieval_quality_config = {\n",
    "    \"input_fields\": [\"input_text\"], \n",
    "    \"context_fields\": [\"local_context\"]\n",
    "}\n",
    "```\n",
    "\n",
    "This configuration is then used to create an `AgenticAIConfiguration` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfe4d9f",
   "metadata": {
    "id": "c3a5d7ac-4af7-4858-a389-ff48b73e6e1f"
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_gov.evaluators.agentic_evaluator import AgenticEvaluator\n",
    "from ibm_watsonx_gov.config import AgenticAIConfiguration\n",
    "from ibm_watsonx_gov.entities.agentic_app import AgenticApp, MetricsConfiguration, Node\n",
    "from ibm_watsonx_gov.metrics import AnswerRelevanceMetric, ContextRelevanceMetric\n",
    "from ibm_watsonx_gov.entities.enums import MetricGroup\n",
    "from ibm_watsonx_gov.config.agentic_ai_configuration import TracingConfiguration\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# Define the metrics to be computed at the agentic app(interaction) level in metrics_configuration under AgenticApp, these metrics use the agent input and output fields.\n",
    "# The node level metrics to be computed after the graph invocation can be specified\n",
    "\n",
    "retrieval_quality_config_web_search_node = {\n",
    "    \"input_fields\": [\"input_text\"],\n",
    "    \"context_fields\": [\"web_context\"],\n",
    "}\n",
    "\n",
    "nodes = [\n",
    "    Node(\n",
    "        name=\"Web \\nSearch \\nNode\",\n",
    "        metrics_configurations=[\n",
    "            MetricsConfiguration(\n",
    "                configuration=AgenticAIConfiguration(\n",
    "                    **retrieval_quality_config_web_search_node\n",
    "                ),\n",
    "                metrics=[ContextRelevanceMetric()],\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "agent_app = AgenticApp(\n",
    "    name=\"Rag agent\",\n",
    "    metrics_configuration=MetricsConfiguration(\n",
    "        metrics=[AnswerRelevanceMetric()], metric_groups=[MetricGroup.CONTENT_SAFETY]\n",
    "    ),\n",
    "    nodes=nodes,\n",
    ")\n",
    "\n",
    "\n",
    "evaluator = AgenticEvaluator(\n",
    "    agentic_app=agent_app,\n",
    "    tracing_configuration=TracingConfiguration(\n",
    "        project_id=os.getenv(\"WATSONX_PROJECT_ID\")\n",
    "    ),\n",
    ")\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "print(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dfd839",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f9fdff; padding:10px; border-radius:6px;\">\n",
    "\n",
    "### [Optional] Set up LLM Judge for metrics evaluation\n",
    "\n",
    "For using LLM as judge when evaluating metrics, you need to define the model provider and the model name along with the needed credentials. The metrics that support evaluation using LLM judge are \n",
    "`context_relevance`, `faithfulness`, `answer_relevance` and `answer_similarity`.\n",
    "\n",
    "To use LLM judge to evaluate a metric you need to add the details of the `llm_judge` when creating the metric object. For example:\n",
    "\n",
    "```python\n",
    "# Define LLM Judge using watsonx.ai\n",
    "llm_judge = LLMJudge(\n",
    "    model=WxAIFoundationModel(\n",
    "        model_id=\"ibm/granite-3-3-8b-instruct\",\n",
    "        project_id=os.environ[\"WATSONX_PROJECT_ID\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Defining LLM Judge using OpenAI\n",
    "llm_judge = LLMJudge(\n",
    "    model=OpenAIFoundationModel(\n",
    "        model_id=\"gpt-4o-mini\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Specify the LLM judge when initializing the metric\n",
    "@evaluator.evaluate_context_relevance(\n",
    "    configuration=AgenticAIConfiguration(**context_relevance_config_local_search_node),\n",
    "    metrics=[ContextRelevanceMetric(llm_judge=llm_judge)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08fea42",
   "metadata": {
    "id": "3ac68caa-593b-4403-bc88-c4ce4659d91f"
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_gov.entities.foundation_model import WxAIFoundationModel\n",
    "from ibm_watsonx_gov.entities.llm_judge import LLMJudge\n",
    "\n",
    "# _set_env(\"PROJECT_ID\")\n",
    "PROJECT_ID = os.environ[\"WATSONX_PROJECT_ID\"]\n",
    "\n",
    "llm_judge = LLMJudge(\n",
    "    model=WxAIFoundationModel(\n",
    "        model_id=\"ibm/granite-3-3-8b-instruct\",\n",
    "        project_id=PROJECT_ID,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90ae904",
   "metadata": {},
   "source": [
    "### Build your langgraph application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a1224f",
   "metadata": {
    "id": "203e2490-a86f-4d28-89fa-64a1a643d513"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langgraph.config import RunnableConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77583a8e",
   "metadata": {},
   "source": [
    "#### Define vector database retrieval node\n",
    "\n",
    "We are using a Similarity with Threshold Retrieval strategy. This will fetch the top 3 documents matching the query if the threshold score is more than 0.1\n",
    "\n",
    "The `local_search_node` tool defined below is decorated with IBM watsonx.governance evaluator group `evaluate_retrieval_quality` tool to compute the retrieval quality metrics. Retrieval Quality metrics include Context Relevance, Retrieval Precision, Average Precision, Hit Rate, Reciprocal Rank, NDCG. Users can use the individual metrics by decorating the individual evaluators (`evaluate_retrieval_precision`, `evaluate_average_precision`, `evaluate_hit_rate`, `evaluate_reciprocal_rank`, `evaluate_ndcg`, `evaluate_context_relevance`) . This node reads the user query from the `input_text` attribute from the application state and writes the result into the `local_context` attribute back to the application state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82a78b8",
   "metadata": {
    "id": "6fb6ab56-8ed9-4ae1-bf0f-2e10779cd8a6"
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_gov.metrics import ContextRelevanceMetric\n",
    "\n",
    "\n",
    "def get_local_search_node(evaluator):\n",
    "    retrieval_quality_config_local_search_node = {\n",
    "        \"input_fields\": [\"input_text\"],\n",
    "        \"context_fields\": [\"local_context\"],\n",
    "    }\n",
    "\n",
    "    @evaluator.evaluate_retrieval_quality(\n",
    "        configuration=AgenticAIConfiguration(\n",
    "            **retrieval_quality_config_local_search_node\n",
    "        ),\n",
    "        # Uncomment the following line to evaluate context relevance using LLM as judge\n",
    "        # metrics=[ContextRelevanceMetric(llm_judge=llm_judge)],\n",
    "    )\n",
    "    # @evaluator.evaluate_retrieval_precision(configuration=AgenticAIConfiguration(**retrieval_quality_config_local_search_node))\n",
    "    # @evaluator.evaluate_average_precision(configuration=AgenticAIConfiguration(**retrieval_quality_config_local_search_node))\n",
    "    # @evaluator.evaluate_hit_rate(configuration=AgenticAIConfiguration(**retrieval_quality_config_local_search_node))\n",
    "    # @evaluator.evaluate_reciprocal_rank(configuration=AgenticAIConfiguration(**retrieval_quality_config_local_search_node))\n",
    "    # @evaluator.evaluate_ndcg(configuration=AgenticAIConfiguration(**retrieval_quality_config_local_search_node))\n",
    "    # @evaluator.evaluate_context_relevance(configuration=AgenticAIConfiguration(**retrieval_quality_config_local_search_node))\n",
    "    def local_search_node(state: GraphState, config: RunnableConfig) -> dict:\n",
    "        similarity_threshold_retriever = vector_store.as_retriever(\n",
    "            search_type=\"similarity_score_threshold\",\n",
    "            search_kwargs={\"k\": 3, \"score_threshold\": 0.1},\n",
    "        )\n",
    "        context = similarity_threshold_retriever.invoke(state[\"input_text\"])\n",
    "        print(\"\\n##########Going to the vector search node#############\")\n",
    "        print(\"\\n########## Retrieved Context: #############\")\n",
    "        for i, doc in enumerate(context):\n",
    "            print(f\"\\n--- Document {i} ---\\n{doc.page_content}\\n\")\n",
    "\n",
    "        return {\"local_context\": [doc.page_content for doc in context]}\n",
    "\n",
    "    return local_search_node\n",
    "\n",
    "\n",
    "local_search_node = get_local_search_node(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac72b2fb",
   "metadata": {},
   "source": [
    "#### Define web search retrieval node\n",
    "\n",
    "We are using a DuckDuckGo to do the web search.\n",
    "\n",
    "The `web_search_node` tool defined below is decorated with IBM watsonx.governance evaluator group `evaluate_retrieval_quality` tool to compute the retrieval quality metrics. Retrieval Quality metrics include Context Relevance, Retrieval Precision, Average Precision, Hit Rate, Reciprocal Rank, NDCG. Users can use the individual metrics by decorating the individual evaluators as mentioned above. This node reads the user query from the `input_text` attribute from the application state and writes the result into the `web_context` attribute back to the application state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46d1353",
   "metadata": {
    "id": "7160253b-a2e4-4af8-a226-b00e9535804e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ddgs import DDGS\n",
    "\n",
    "\n",
    "def get_web_search_node(evaluator):\n",
    "    retrieval_quality_config_web_search_node = {\n",
    "        \"input_fields\": [\"input_text\"],\n",
    "        \"context_fields\": [\"web_context\"],\n",
    "    }\n",
    "\n",
    "    @evaluator.evaluate_retrieval_quality(\n",
    "        configuration=AgenticAIConfiguration(\n",
    "            **retrieval_quality_config_web_search_node\n",
    "        ),\n",
    "        # Uncomment the following line to evaluate context relevance using LLM as judge\n",
    "        # metrics=[ContextRelevanceMetric(llm_judge=llm_judge)],\n",
    "    )\n",
    "    def web_search_node(state: GraphState, max_results: int = 5) -> dict:\n",
    "        \"\"\"Perform web search using DuckDuckGo and return relevant content snippets\"\"\"\n",
    "        query = state[\"input_text\"]\n",
    "        results = []\n",
    "        print(\"\\n##########Going to the web search node#############\")\n",
    "        with DDGS() as ddgs:\n",
    "            search_results = ddgs.text(query, max_results=max_results)\n",
    "\n",
    "            for result in search_results:\n",
    "                url = result.get(\"href\")\n",
    "                snippet = result.get(\"body\") or result.get(\"title\")\n",
    "                if snippet and url:\n",
    "                    results.append(f\"From {url}: {snippet}\")\n",
    "                    print(\n",
    "                        \"\\n###HERE IS A WEB SEARCH SNIPPET####\\n\",\n",
    "                        snippet,\n",
    "                        \"\\n###END OF WEB SNIPPET###\\n\",\n",
    "                    )\n",
    "\n",
    "        return {\"web_context\": results[:max_results]}\n",
    "\n",
    "    return web_search_node\n",
    "\n",
    "\n",
    "web_search_node = get_web_search_node(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a966c40e",
   "metadata": {},
   "source": [
    "### Create a prompt template to get the response from the LLM\n",
    "\n",
    "We are using a Llama model from watsonx to generate an answer for our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be504af6",
   "metadata": {
    "id": "80da4e5a-1125-46b5-8475-a194ff5b9ad3"
   },
   "outputs": [],
   "source": [
    "def generate_response(input_text: str, context_text: list[str]):\n",
    "    from langchain_ibm import WatsonxLLM\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "    generate_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Answer the query in 1 sentence using only the provided context:\\n\"\n",
    "        \"Context: {context}\\n\"\n",
    "        \"Question: {input_text}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    formatted_prompt = generate_prompt.invoke(\n",
    "        {\"input_text\": input_text, \"context\": \"\\n\".join(context_text)}\n",
    "    )\n",
    "\n",
    "    # Initialize WatsonX LLM\n",
    "    llm = WatsonxLLM(\n",
    "        model_id=\"meta-llama/llama-3-3-70b-instruct\",  # You can change this to other available models\n",
    "        url=\"https://us-south.ml.cloud.ibm.com\",  # Update with your WatsonX URL\n",
    "        project_id=os.getenv(\n",
    "            \"WATSONX_PROJECT_ID\"\n",
    "        ),  # Replace with your actual project ID\n",
    "        params={\n",
    "            \"max_new_tokens\": 500,\n",
    "            \"decoding_method\": \"greedy\",\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"stop_sequences\": [\".\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    result = llm.invoke(formatted_prompt)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99411248",
   "metadata": {},
   "source": [
    "#### Define vector-database based answer generation tool\n",
    "\n",
    "We are using the watsonx call in `generate_response` to generate an answer for our query.\n",
    "\n",
    "\n",
    "The `generate_local_context_node` defined below is decorated with evaluator group `evaluate_answer_quality` for computing answer quality metrics. Answer Quality metrics include Answer Relevance, Faithfulness, Answer Similarity, Unsuccessful Requests. Like in the previous cell, this node reads the user query from the `input_text` attribute from the application state, the `context` attribute consists of the context chunks. After generating the answer, the node writes the result into the `generated_text` attribute back to the application state. Users can use the individual metrics by decorating the individual evaluators as mentioned above. Similar to retrieval quality metrics in the previous cell, answer quality metrics can be evaluated using LLM as judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a2dec2",
   "metadata": {
    "id": "aff376ef-7dd2-4679-bdfa-672b977aa3dc"
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_gov.metrics import FaithfulnessMetric\n",
    "\n",
    "\n",
    "def get_local_context_node(evaluator):\n",
    "    answer_quality_config_local_context_node = {\n",
    "        \"input_fields\": [\"input_text\"],\n",
    "        \"context_fields\": [\"local_context\"],\n",
    "        \"output_fields\": [\"generated_text\"],\n",
    "        \"reference_fields\": [\"ground_truth\"],\n",
    "    }\n",
    "\n",
    "    @evaluator.evaluate_answer_quality(\n",
    "        configuration=AgenticAIConfiguration(\n",
    "            **answer_quality_config_local_context_node\n",
    "        ),\n",
    "        # Uncomment the following block evaluate the faithfulness metric using LLM judge\n",
    "        # metrics=[FaithfulnessMetric(llm_judge=llm_judge)],\n",
    "    )\n",
    "    # @evaluator.evaluate_faithfulness(configuration=AgenticAIConfiguration(**answer_quality_config_local_context_node))\n",
    "    # @evaluator.evaluate_answer_similarity(configuration=AgenticAIConfiguration(**answer_quality_config_local_context_node))\n",
    "    # @evaluator.evaluate_answer_relevance(configuration=AgenticAIConfiguration(**answer_quality_config_local_context_node))\n",
    "    # @evaluator.evaluate_unsuccessful_requests(configuration=AgenticAIConfiguration(**answer_quality_config_local_context_node))\n",
    "    def generate_local_context_node(state: GraphState, config: RunnableConfig) -> dict:\n",
    "\n",
    "        result = generate_response(state[\"input_text\"], state[\"local_context\"])\n",
    "\n",
    "        # print (\"\\n####################\\n I am inside the vectordb tool\\n####################\\n\")\n",
    "        # print (result)\n",
    "        return {**state, \"generated_text\": result}\n",
    "\n",
    "    return generate_local_context_node\n",
    "\n",
    "\n",
    "generate_local_context_node = get_local_context_node(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da149de",
   "metadata": {},
   "source": [
    "#### Define web-search based answer generation tool\n",
    "\n",
    "We are using the watsonx call in `generate_response` to generate an answer for our query.\n",
    "\n",
    "The `generate_web_context_node` defined below is decorated with with evaluator group `evaluate_answer_quality` for computing answer quality metrics. Answer Quality metrics include Answer Relevance, Faithfulness, Answer Similarity, Unsuccessful Requests. Like in the previous cell, this node reads the user query from the `input_text` attribute from the application state, the `context` attribute consists of the context chunks. After generating the answer, the node writes the result into the `generated_text` attribute back to the application state. Users can use the individual metrics by decorating the individual evaluators as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239d70b4",
   "metadata": {
    "id": "aeeb14d6-7e5a-4e41-a042-7b79fa3bed60"
   },
   "outputs": [],
   "source": [
    "def get_web_context_node(evaluator):\n",
    "    answer_quality_config_web_context_node = {\n",
    "        \"input_fields\": [\"input_text\"],\n",
    "        \"context_fields\": [\"web_context\"],\n",
    "        \"output_fields\": [\"generated_text\"],\n",
    "        #    \"reference_fields\":[\"ground_truth\"]\n",
    "    }\n",
    "\n",
    "    @evaluator.evaluate_answer_quality(\n",
    "        configuration=AgenticAIConfiguration(**answer_quality_config_web_context_node)\n",
    "    )\n",
    "    def generate_web_context_node(state: GraphState, config: RunnableConfig) -> dict:\n",
    "\n",
    "        result = generate_response(state[\"input_text\"], state[\"web_context\"])\n",
    "        # print (\"\\n####################\\n I am inside the websearch tool\\n####################\\n\")\n",
    "        # print (result)\n",
    "        return {**state, \"generated_text\": result}\n",
    "\n",
    "    return generate_web_context_node\n",
    "\n",
    "\n",
    "generate_web_context_node = get_web_context_node(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eaf890",
   "metadata": {},
   "source": [
    "#### Adding a Router Function to Check Context Relevance\n",
    "\n",
    "- The `check_context_relevance` function evaluates the retrieved context and assigns a **Context Relevance Score**.  \n",
    "- If the score meets the required threshold, the workflow proceeds to the **Vector DB Answer Generation** node.  \n",
    "- If the score is below the threshold, the workflow reroutes to the **Web Search Node** for additional information.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b26df9",
   "metadata": {
    "id": "58585e14-02ca-4cf6-83f9-35dc6543ff43"
   },
   "outputs": [],
   "source": [
    "def check_context_relevance(state: GraphState, config: RunnableConfig) -> str:\n",
    "\n",
    "    # Filter results for \"context_relevance\" from the \"retrieval_node\"\n",
    "    latest_metric = evaluator.get_metric_result(\n",
    "        metric_name=\"context_relevance\", node_name=\"local_search_node\"\n",
    "    )\n",
    "    print(\n",
    "        \"\\n######## Context Relevance Metric: ###########\\n\",\n",
    "        latest_metric.value,\n",
    "    )\n",
    "    if not latest_metric:\n",
    "        print(\n",
    "            \"\\n######## NO Context Relevance Metric: ###########\\n\",\n",
    "            latest_metric.value,\n",
    "        )\n",
    "        # Default to \"no\" if no metrics found\n",
    "        return \"Context Relevance \\nScore is Bad\"\n",
    "\n",
    "    # Check if context relevance is below threshold\n",
    "    if latest_metric.value > 0.35:\n",
    "        print(\n",
    "            \"\\n######## GOOD Context Relevance Metric: ###########\\n\",\n",
    "            latest_metric.value,\n",
    "        )\n",
    "        return \"Context Relevance \\nScore is Good\"\n",
    "    else:\n",
    "        print(\n",
    "            \"\\n######## BAD Context Relevance Metric: ###########\\n\",\n",
    "            latest_metric.value,\n",
    "        )\n",
    "        return \"Context Relevance \\nScore is Bad\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b1c374",
   "metadata": {},
   "source": [
    "#### Assemble your application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26016fc",
   "metadata": {
    "id": "da6bc1d5-bee1-4aeb-bee1-0b4ed68364e9"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "\n",
    "\n",
    "def build_llm_agent():\n",
    "    graph = StateGraph(GraphState)\n",
    "\n",
    "    # Add nodes\n",
    "    graph.add_node(\"Vector DB \\nRetrieval \\nNode\", local_search_node)\n",
    "    graph.add_node(\"Web \\nSearch \\nNode\", web_search_node)\n",
    "    graph.add_node(\"Vector DB \\nAnswer \\nGeneration\", generate_local_context_node)\n",
    "    graph.add_node(\"Web Search \\nAnswer \\nGeneration\", generate_web_context_node)\n",
    "\n",
    "    # Add edges\n",
    "    graph.add_edge(START, \"Vector DB \\nRetrieval \\nNode\")\n",
    "    graph.add_conditional_edges(\n",
    "        \"Vector DB \\nRetrieval \\nNode\",\n",
    "        check_context_relevance,\n",
    "        {\n",
    "            \"Context Relevance \\nScore is Good\": \"Vector DB \\nAnswer \\nGeneration\",\n",
    "            \"Context Relevance \\nScore is Bad\": \"Web \\nSearch \\nNode\",\n",
    "        },\n",
    "    )\n",
    "    graph.add_edge(\"Web \\nSearch \\nNode\", \"Web Search \\nAnswer \\nGeneration\")\n",
    "    graph.add_edge(\"Vector DB \\nAnswer \\nGeneration\", END)\n",
    "    graph.add_edge(\"Web Search \\nAnswer \\nGeneration\", END)\n",
    "\n",
    "    # Compile the graph\n",
    "    rag_app = graph.compile()\n",
    "\n",
    "    return rag_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2466925",
   "metadata": {
    "id": "34997737-6809-4a4c-b5e0-7f83f7b52a69"
   },
   "outputs": [],
   "source": [
    "rag_app = build_llm_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f88f915",
   "metadata": {},
   "source": [
    "#### Display the graph\n",
    "\n",
    "**Note:** you can get the link from below and paste it to https://mermaid.live to see the graph structure. To see the graph image, follow these steps: \n",
    "\n",
    "- Copy the entire printed text.\n",
    "\n",
    "- Open https://mermaid.live\n",
    "\n",
    "- Paste it in the editor.\n",
    "\n",
    "The diagram will render instantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867dfcb7",
   "metadata": {
    "id": "f0218545-fc8e-4206-8681-2e860c352574"
   },
   "outputs": [],
   "source": [
    "# # Get the raw Mermaid graph syntax\n",
    "# mermaid_code = rag_app.get_graph().draw_mermaid()\n",
    "\n",
    "# # Print it so you can copy-paste into mermaid.ink or mermaid.live\n",
    "# print(mermaid_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1063d8",
   "metadata": {},
   "source": [
    " <div style=\"background-color:#dff6dd; padding:10px; border-radius:6px;\">\n",
    "  <h3 style=\"margin:0;\">\n",
    "  \n",
    "  🤔 Discussion Point: \n",
    "  \n",
    "Here we used a fixed threshold rather than a LLM as a judge. What are the use cases for which a fixed threhsold won't be a good fit?\n",
    "\n",
    "Think about a scenario that would require LLM as a judge to route the agent.  </h3>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9489c57",
   "metadata": {},
   "source": [
    "## 4. Run Agent With Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b890cb87",
   "metadata": {},
   "source": [
    "### Do a single invocation\n",
    "\n",
    "Now the application is invoked for a single row of data. You will see a new key as the input:\n",
    "\n",
    "`ground_truth`: As the name suggests, this attribute holds the ground truth for your input text. This is needed for the answer similarity metric, which is a reference based metric. For the other metrics, this is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17970aac",
   "metadata": {
    "id": "8752aa90-07a7-47ae-8662-ee67ab432864",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Task exception was never retrieved\")\n",
    "\n",
    "evaluator.start_run()\n",
    "result = rag_app.invoke(\n",
    "    {\n",
    "        \"input_text\": \"how can I minimize my taxes?\",\n",
    "        \"ground_truth\": \"To minimize taxes, consider the following strategies based on the provided context: 1.Adjust withholding: Review your withholding and update it if necessary to avoid underpayment or overpayment. This can help prevent surprise tax bills later. 2.Maximize tax-advantaged accounts: Contribute to accounts like Health Savings Accounts (HSAs), Flexible Spending Accounts (FSAs), or 529 plans. These accounts offer tax advantages that can add up over time. 3.Organize tax documents: Keep records of deductions, receipts, and other important financial paperwork. This can help you identify potential tax deductions and ensure you're not missing out on any tax-saving opportunities. 4.Automate saving and investing: An automated approach can help take the emotion out of investing and create a disciplined saving process, reducing the chance of impulsive changes that might lead to higher taxes. 5.Continue financial education: Stay informed about changes in tax rules and policies that could impact your ta situation. This can help you make proactive decisions to minimize your tax liability.\",\n",
    "        \"record_id\": \"764545\",\n",
    "    }\n",
    ")\n",
    "\n",
    "evaluator.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a22a5b",
   "metadata": {},
   "source": [
    "### Prepare the app results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faed61c",
   "metadata": {
    "id": "f9deca6f-3144-43af-8ad0-311b5fd0eba8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.keys()\n",
    "for key, val in result.items():\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"{key=}\")\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7111ed63",
   "metadata": {
    "id": "bec85a52-24c9-45ac-909a-a86fe4fa5848"
   },
   "outputs": [],
   "source": [
    "eval_result = evaluator.get_result()\n",
    "\n",
    "metric_result = eval_result.to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5033e99f",
   "metadata": {},
   "source": [
    "### Display the metric results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa536580",
   "metadata": {
    "id": "0bee3b70-801e-4bc2-8138-8ed708d67ef4"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "display(metric_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0295155",
   "metadata": {},
   "source": [
    "#### Get Metric Results for a specific node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01639f67",
   "metadata": {
    "id": "cc67c214-ce27-40d7-b7f5-c1703f674d5a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_result.get_aggregated_metrics_results(node_name=\"Web Search \\nAnswer \\nGeneration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea32183",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e6f7ff; padding:20px; border-radius:10px;\n",
    "            border: 2px solid #3399ff; text-align:left; \n",
    "            display:inline-block;\">\n",
    "\n",
    "  <h1 style=\"margin-top:0;\">🎉 🏆 🥳 Congratulations!</h1>\n",
    "\n",
    "  <p style=\"font-size:18px;\">\n",
    "You have completed design time evaluations for an advanced LangGraph question-answering agent that uses local documents and web search to answer a given question.\n",
    "  </p>\n",
    "\n",
    "  <p style=\"font-size:16px; color:#444;\">\n",
    "  If you are interested to continue, in the following sections, you can explore \n",
    "  <b>How to evaluate your agent using batch invocation</b> and \n",
    "  <b>Compare the results of different experiments in Watsonx Evaluation Studio</b>.\n",
    "  </p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d2050b",
   "metadata": {},
   "source": [
    "### Invoke the graph on multiple rows\n",
    "\n",
    "IBM watsonx.governance evaluation of Agentic Applications can be done with batch invocation too. Here, a dataframe with questions and ground truths for those questions have been defined. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ddf9e3",
   "metadata": {},
   "source": [
    "#### Get a set of test questions for the RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b351fc6",
   "metadata": {
    "id": "2c2d2c7a-8c2e-476b-b176-820e502b4415"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# supply a CSV with two columns: input_text, ground_truth\n",
    "# input_text should be questions for the RAG app, ground_truth is examples of \"good\" or expected answers to the RAG query\n",
    "\n",
    "question_bank_df = pd.read_csv(\n",
    "    TEST_QUESTIONS_CSV_URL_1,\n",
    "    encoding=\"ISO-8859-1\",  # or try \"cp1252\" this is for CSV files\n",
    ")\n",
    "\n",
    "question_bank_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1447ce6",
   "metadata": {},
   "source": [
    "## 5. Enable Experiment Tracking And Compare Agent Runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b812e7",
   "metadata": {
    "id": "f938541f-0738-4492-a7b4-f03b354c9375"
   },
   "outputs": [],
   "source": [
    "ai_experiment_id = evaluator.track_experiment(\n",
    "    name=\"Agentic Evaluation\", use_existing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedd986f",
   "metadata": {},
   "source": [
    "### Start the Experiment Run 1\n",
    "Now the application is invoked for the first three rows of data with the input_text which is the user's raw input query or question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dff328",
   "metadata": {
    "id": "e6e757d0-7c9e-4f35-a59b-e231db06bf28",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from ibm_watsonx_gov.entities.ai_experiment import AIExperimentRunRequest\n",
    "\n",
    "name = \"1st Agent Run - First 3 Questions\"\n",
    "# [OPTIONAL] Specify custom tags for the experiment run\n",
    "custom_tags = []\n",
    "\"\"\"\n",
    "custom_tags = [\n",
    "    {\n",
    "        \"key\": \"LLM\",\n",
    "        \"value\": \"garbage\"\n",
    "    },\n",
    "    {\n",
    "        \"key\": \"temperature\",\n",
    "        \"value\": 0.1\n",
    "    }\n",
    "]\n",
    "\"\"\"\n",
    "run_request = AIExperimentRunRequest(name=name, custom_tags=custom_tags)\n",
    "evaluator.start_run(run_request)\n",
    "\n",
    "results = []\n",
    "for record in question_bank_df.head(3).to_dict(\"records\"):\n",
    "    result = rag_app.batch(inputs=[record])  # Wrap single record in a list\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f3d328",
   "metadata": {
    "id": "8756f544-82e5-4d67-b254-22ff8bb750cd"
   },
   "outputs": [],
   "source": [
    "evaluator.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06cba47",
   "metadata": {},
   "source": [
    "### Prepare the App results and Display the metrics\n",
    "By default, the metric result will only include the interaction_id column.\n",
    "If you want to include additional data like input, output or ground_truth, you can specify them in the input_data parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761cdf6",
   "metadata": {
    "id": "e5e66d10-498a-4d2a-be5b-9e96f49ac6b3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if isinstance(results, list):\n",
    "    for result in results:\n",
    "        print(\"#\" * 20)\n",
    "        for key, val in result[0].items():\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"{key=}\")\n",
    "            print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469267c8",
   "metadata": {
    "id": "a8b469f9-dcdb-4df4-ab8b-0d308a083f38"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "#### ONE row per metric row ####\n",
    "\n",
    "# Make sure result_df has 1 row\n",
    "result_df = pd.DataFrame([result[0]])\n",
    "\n",
    "input_data = result_df[[\"input_text\", \"generated_text\"]]\n",
    "\n",
    "eval_result = evaluator.get_result()\n",
    "metric_result = eval_result.to_df()\n",
    "\n",
    "# Repeat input_data for each row of metric_result\n",
    "input_repeated = input_data.loc[\n",
    "    input_data.index.repeat(len(metric_result))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Combine\n",
    "result_with_data = pd.concat(\n",
    "    [input_repeated, metric_result.reset_index(drop=True)], axis=1\n",
    ")\n",
    "\n",
    "display(result_with_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b9a24a",
   "metadata": {},
   "source": [
    "### Start the Experiment Run 2\n",
    "Now the application is invoked to evaluate the second run of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f922f6",
   "metadata": {
    "id": "be403470-8f5f-4dd6-9d0b-7c181f1b6fdb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "name = \"2nd Agent Run - Last 3 Question\"\n",
    "# [OPTIONAL] Specify custom tags for the experiment run\n",
    "custom_tags = []\n",
    "\"\"\"\n",
    "custom_tags = [\n",
    "    {\n",
    "        \"key\": \"LLM\",\n",
    "        \"value\": \"Granite 20B\"\n",
    "    },\n",
    "    {\n",
    "        \"key\": \"temperature\",\n",
    "        \"value\": 0.5\n",
    "    }\n",
    "]\n",
    "\"\"\"\n",
    "run_request = AIExperimentRunRequest(name=name, custom_tags=custom_tags)\n",
    "evaluator.start_run(run_request)\n",
    "\n",
    "results = []\n",
    "for record in question_bank_df.tail(3).to_dict(\"records\"):\n",
    "    result = rag_app.batch(inputs=[record])  # Wrap single record in a list\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cade96",
   "metadata": {
    "id": "9c68fe60-cd7a-426d-9be2-dc6576e1c9e8"
   },
   "outputs": [],
   "source": [
    "evaluator.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97635a16",
   "metadata": {
    "id": "92b1f2c2-bc62-4b40-a7ff-beb583da0067"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "result_df = pd.DataFrame(result)\n",
    "input_data = result_df[\n",
    "    [\"input_text\", \"generated_text\"]\n",
    "]  # Add the columns which should be part of the application metric results: \"local_context\", \"web_context\"\n",
    "eval_result = evaluator.get_result()\n",
    "metric_result = eval_result.to_df()\n",
    "result_with_data = pd.concat([input_data, metric_result], axis=1)\n",
    "\n",
    "display(result_with_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0232dc",
   "metadata": {},
   "source": [
    "### Compare the AI experiment runs in Evaluation Studio UI \n",
    "You can use Evaluation Studio UI to view the comparison of AI experiment runs with below URL.\n",
    "\n",
    "**Important Note:** When you click the link, you will be directed to a page that requires you to complete a setup before you can view the comparison plot. Please refer to the instructions in the provided PDF for a few simple steps to complete the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6cc6f0",
   "metadata": {
    "id": "c387d44c-c9ec-4034-af37-e12781eca4d6"
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_gov.entities.ai_experiment import AIExperiment\n",
    "\n",
    "ai_experiment = AIExperiment(asset_id=ai_experiment_id)\n",
    "\n",
    "evaluator.compare_ai_experiments(ai_experiments=[ai_experiment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42023b5c",
   "metadata": {
    "id": "ffa50ab2-8011-4b14-b948-7ca38fbcd59c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
